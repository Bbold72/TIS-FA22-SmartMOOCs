{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import nltk\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.dates import MinuteLocator, DateFormatter\n",
    "from sklearn.metrics import silhouette_score\n",
    "from typing import Dict, List\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "from src import utils\n",
    "from src.data import process_transcripts\n",
    "from src.data.make_corpus import Corpus, Vocabulary\n",
    "from src.models.breakpoints import calc_breakpoints, merge_documents_breakpoints\n",
    "\n",
    "\n",
    "ROOT_DIR = utils.get_project_root()\n",
    "DATA_DIR = Path.joinpath(ROOT_DIR, 'data')\n",
    "DATA_RAW_DIR = Path.joinpath(DATA_DIR, 'raw/cs-410')\n",
    "INTERMEDATE_DATA_DIR = Path.joinpath(DATA_DIR, 'intermediate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Transcript File\n",
    "Within each transcript segment, the first line is an integer ID of the segment, the next line is the beginning and end time of the segment, and the rest of the lines is the text of the segment. Each segment is separated by a newline character.\n",
    "\n",
    "#### First 15 lines of raw text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '04_week-4/02_week-4-lessons/01_lesson-4-1-probabilistic-retrieval-model-basic-idea.en.srt'\n",
    "# file_name = '04_week-4/02_week-4-lessons/02_lesson-4-2-statistical-language-model.en.srt'\n",
    "file_path = Path.joinpath(DATA_RAW_DIR, file_name)\n",
    "\n",
    "# print first 10 lines of raw transcript file\n",
    "with open(file_path, 'r') as f:\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        print(line, end='')\n",
    "        i += 1\n",
    "        if i == 15:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Raw Transcript File\n",
    "Process each trancript segment and store in a Segment class. Return a list of segments. Does not process the last segment as it's just music that play at the end of a lesson.\n",
    "\n",
    "#### First three processed transcript Segment classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_segments = process_transcripts.process_transcript(file_path)\n",
    "\n",
    "# print out first 5 segments\n",
    "for s in transcript_segments[:3]:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Corpus\n",
    "- Create a vocabulary set, a unique set of words in the transcript\n",
    "- Combine the raw text segements at certain intervals, $T$, to create approximately $\\frac{Total Time}{T}$ documents. Also, ensure that each document does not end in the middle of a sentence.\n",
    "- Calcuate term-document fequency matrix\n",
    "- Calculate similarity of time series of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_intervals: List[int] = [5, 30, 45, 60]   # time intervals to split transcipt into documents\n",
    "corpus_times: Dict[int, Corpus] = dict()      # store corpus class in dictionary\n",
    "\n",
    "# create unique set of tokens\n",
    "vocab = Vocabulary(transcript_segments, remove_stop_words=True, combine_ngrams=True, stem_words=True)  \n",
    "\n",
    "for interval in time_intervals:\n",
    "    \n",
    "    # merge combines segments by time interval\n",
    "    documents = utils.merge_documents_time_interval(vocab.transcript_segements, interval) \n",
    "\n",
    "    # class to store vocab set, merged documents, and perform calculations on them\n",
    "    corpus = Corpus(vocab, documents)\n",
    "    corpus.create_term_doc_freq_matrix()\n",
    "    corpus.calc_similarity_ts()\n",
    "    \n",
    "    corpus_times[interval] = corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First 3 documents with length of approximately 30 seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(corpus_times[30].documents[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot of Time Series at Different Time Intevals\n",
    "\n",
    "### Cosine Similarity of 5 Second Sequential Transcript Segments\n",
    "- Each transcript segment is approximately 1 to 2 sentences\n",
    "- Cosine similarity is too noisy for such a short time interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_x_values = lambda docs: [d.end for d in docs[1:]]\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(make_x_values(corpus_times[5].documents), corpus_times[5].ts_cos_similarity)\n",
    "plt.ylabel('Cosine Similarity')\n",
    "plt.suptitle('Cosine Similarity of 5-Second Sequential Transcript Segments')\n",
    "plt.title('Week 4 - Lesson-4-1: Probabilistic Retrieval Model Basic Idea')\n",
    "\n",
    "ax.xaxis.set_major_locator(MinuteLocator(byminute=range(0, 60), interval=2))\n",
    "ax.xaxis.set_major_formatter(DateFormatter('%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity of 30, 45, and 60 Second Sequential Transcript Segments\n",
    "- Longer transcript segments reduce variability of time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_x_values = lambda docs: [d.end for d in docs[1:]]\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(make_x_values(corpus_times[30].documents), corpus_times[30].ts_cos_similarity, make_x_values(corpus_times[45].documents), corpus_times[45].ts_cos_similarity, make_x_values(corpus_times[60].documents), corpus_times[60].ts_cos_similarity)\n",
    "plt.legend(['30', '45', '60'], title='Time Intervals (Seconds)')\n",
    "plt.xlabel('Minutes')\n",
    "plt.ylabel('Cosine Similarity')\n",
    "plt.suptitle('Cosine Similarity of Sequential Transcript Segments')\n",
    "plt.title('Week 4 - Lesson-4-1: Probabilistic Retrieval Model Basic Idea')\n",
    "\n",
    "ax.xaxis.set_major_locator(MinuteLocator(byminute=range(0, 60), interval=2))\n",
    "ax.xaxis.set_major_formatter(DateFormatter('%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calc Breakpoints\n",
    "Returns list of Segment indices of breakpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtopics_corpuses: Dict[int, Corpus] = dict()\n",
    "breakpoints_list: Dict[int, Corpus] = dict()\n",
    "for interval in time_intervals:\n",
    "    corpus = corpus_times[interval]\n",
    "    breakpoints = calc_breakpoints(corpus.ts_cos_similarity)\n",
    "    print(f'Breakpoints at {interval}-Second Interval:', breakpoints)\n",
    "    breakpoints_list[interval] = breakpoints\n",
    "    subtopics_documents = merge_documents_breakpoints(corpus.documents, breakpoints)\n",
    "    topic_transitions_corpus = Corpus(corpus.vocab, subtopics_documents)\n",
    "    topic_transitions_corpus.create_term_doc_freq_matrix()\n",
    "    topic_transitions_corpus.calc_similarity_ts()\n",
    "\n",
    "    # add to dict\n",
    "    subtopics_corpuses[interval] = topic_transitions_corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Breakpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize = (14, 4))\n",
    "for i, interval in enumerate(time_intervals[1:]):   # skip 5 second interval\n",
    "    axes[i].plot(make_x_values(corpus_times[interval].documents), corpus_times[interval].ts_cos_similarity)\n",
    "    axes[i].vlines([corpus_times[interval].documents[b].end for b in breakpoints_list[interval]], [0] * len(breakpoints_list[interval]), [1] * len(breakpoints_list[interval]), colors='k', linestyles='dashed')\n",
    "    axes[i].set_ylim(0, 0.65)\n",
    "    axes[i].set_xlim(datetime.datetime(1900, 1, 1, 0, 0, 0), datetime.datetime(1900, 1, 1, 0, 13, 0),)\n",
    "    axes[i].set_xlabel('Minutes')\n",
    "    axes[i].set_ylabel('Cosine Similarity')\n",
    "    plt.suptitle('Cosine Similarity of Sequential Transcript Segments with Detected Breakpoints')\n",
    "    axes[i].set_title(f'Time Interval: {interval} Seconds')\n",
    "\n",
    "    axes[i].xaxis.set_major_locator(MinuteLocator(byminute=range(0, 60), interval=2))\n",
    "    axes[i].xaxis.set_major_formatter(DateFormatter('%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Breakpoints\n",
    "Calculate silhouette Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide raw transcript into 10 second intervals\n",
    "base_corpus = Corpus(vocab, utils.merge_documents_time_interval(vocab.transcript_segements, 10))\n",
    "base_corpus.create_term_doc_freq_matrix()\n",
    "\n",
    "# create a mapping from 10 second interval to 60 second intervals\n",
    "naive_time_label_map = utils.make_segment_label_mapping(base_corpus.documents, corpus_times[60].documents)\n",
    "print('Naive, 60-Second Topic Transition:', silhouette_score(base_corpus.term_doc_freq_matrix.T, naive_time_label_map)) # naive, 60 second topic transition score\n",
    "print()\n",
    "\n",
    "for interval in time_intervals:\n",
    "\n",
    "    # create a mapping from 10 second intervals to model predicted topic transitions\n",
    "    subtopic_label_map = utils.make_segment_label_mapping(base_corpus.documents, subtopics_corpuses[interval].documents)\n",
    "\n",
    "    # calculate silhouettes scores\n",
    "    print(f'Estimated Breakpoints for {interval}-Second Intervals:', silhouette_score(base_corpus.term_doc_freq_matrix.T, subtopic_label_map))   # model topic transition score\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in subtopics_corpuses[60].documents:\n",
    "    print('Segment ID:', doc.id)\n",
    "    print(f'Interval: {doc.beg} --> {doc.end}')\n",
    "    print('Text:')\n",
    "    print(doc.text)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('tis-project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85a5c2dc979ec021335ff2758cc7fbdc3d2baa50f6cf88c303f22e6e76e98821"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
