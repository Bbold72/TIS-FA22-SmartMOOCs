{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import nltk\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.dates import MinuteLocator, DateFormatter\n",
    "from sklearn.metrics import silhouette_score\n",
    "from typing import Dict, List\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "from src import utils\n",
    "from src.data import process_transcripts\n",
    "from src.data.make_corpus import Corpus, Vocabulary\n",
    "from src.models.breakpoints import calc_breakpoints, merge_documents_breakpoints\n",
    "\n",
    "\n",
    "ROOT_DIR = utils.get_project_root()\n",
    "DATA_DIR = Path.joinpath(ROOT_DIR, 'data')\n",
    "DATA_RAW_DIR = Path.joinpath(DATA_DIR, 'raw/cs-410')\n",
    "INTERMEDATE_DATA_DIR = Path.joinpath(DATA_DIR, 'intermediate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Transcript File\n",
    "Within each transcript segment, the first line is and integer ID of the segment, the next line is the beginning and end time of the segment, and the rest of the lines is the text of the segment. Each segment is separated by a newlinw character.\n",
    "\n",
    "#### First 15 lines of raw text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '04_week-4/02_week-4-lessons/01_lesson-4-1-probabilistic-retrieval-model-basic-idea.en.srt'\n",
    "file_path = Path.joinpath(DATA_RAW_DIR, file_name)\n",
    "\n",
    "# print first 10 lines of raw transcript file\n",
    "with open(file_path, 'r') as f:\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        print(line, end='')\n",
    "        i += 1\n",
    "        if i == 15:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Raw Transcript File\n",
    "Process each trancript segment and store in a Segment class. Return a list of segments. Does not process the last segment as it's just music that play at the end of a lesson.\n",
    "\n",
    "#### First three processed transcript Segment classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_segments = process_transcripts.process_transcript(file_path)\n",
    "\n",
    "# print out first 5 segments\n",
    "for s in transcript_segments[:3]:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Corpus\n",
    "- Create a vocabulary set, a unique set of words in the transcript\n",
    "- Combine the raw text segements at certain intervals, $T$, to create approximately $\\frac{Total Time}{T}$ documents. Also, ensure that each document does not end in the middle of a sentence.\n",
    "- Calcuate term-document fequency matrix\n",
    "- Calculate similarity of time series of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_intervals: List[int] = [5, 30, 45, 60]   # time intervals to split transcipt into documents\n",
    "corpus_times: Dict[int, Corpus] = dict()   # store corpus class in dictionary\n",
    "\n",
    "# create unique set of tokens\n",
    "vocab = Vocabulary(transcript_segments, remove_stop_words=True, combine_ngrams=True, stem_words=True)  \n",
    "\n",
    "for interval in time_intervals:\n",
    "    # merge combines segments by time interval\n",
    "    documents = utils.merge_documents_time_interval(vocab.transcript_segements, interval) \n",
    "\n",
    "    # class to store vocab set, merged documents, and perform calculations on them\n",
    "    corpus = Corpus(vocab, documents)\n",
    "    corpus.create_term_doc_freq_matrix()\n",
    "    corpus.calc_similarity_ts()\n",
    "    \n",
    "    corpus_times[interval] = corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First 3 documents with length of approximately 30 seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(corpus_times[30].documents[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot of Time Series at Different Time Intevals\n",
    "\n",
    "### Cosine Similarity of 5 Second Sequential Transcript Segments\n",
    "- Each transcript segment is approximately 1 to 2 sentences\n",
    "- Cosine similarity is too noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_x_values = lambda docs: [d.end for d in docs[1:]]\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(make_x_values(corpus_times[5].documents), corpus_times[5].ts_cos_similarity)\n",
    "plt.ylabel('Cosine Similarity')\n",
    "plt.suptitle('Cosine Similarity of 5-Second Sequential Transcript Segments')\n",
    "plt.title('Week 4 - Lesson-4-1: Probabilistic Retrieval Model Basic Idea')\n",
    "\n",
    "ax.xaxis.set_major_locator(MinuteLocator(byminute=range(0, 60), interval=2))\n",
    "ax.xaxis.set_major_formatter(DateFormatter('%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity of 30, 45, and 60 Second Sequential Transcript Segments\n",
    "- Longer transcript segments reduce variability of time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make_x_values = lambda idx: [i*idx for i in range(len(corpus_times[idx].ts_cos_similarity))]\n",
    "# make_x_values = lambda idx: [i*idx for i in range(1, len(corpus_times[idx].ts_cos_similarity)+1)]\n",
    "# plt.plot(make_x_values(30), corpus_times[30].ts_cos_similarity, make_x_values(45), corpus_times[45].ts_cos_similarity, make_x_values(60), corpus_times[60].ts_cos_similarity)\n",
    "# plt.legend(['30', '45', '60'], title='Time Intervals (Seconds)')\n",
    "# plt.xlabel('Seconds')\n",
    "# plt.ylabel('Cosine Similarity')\n",
    "# # plt.suptitle('Cosine Similarity of Sequential Transcript Segments')\n",
    "# plt.title('Week 4 - Lesson-4-1: Probabilistic Retrieval Model Basic Idea')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_x_values = lambda docs: [d.end for d in docs[1:]]\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(make_x_values(corpus_times[30].documents), corpus_times[30].ts_cos_similarity, make_x_values(corpus_times[45].documents), corpus_times[45].ts_cos_similarity, make_x_values(corpus_times[60].documents), corpus_times[60].ts_cos_similarity)\n",
    "plt.legend(['30', '45', '60'], title='Time Intervals (Seconds)')\n",
    "plt.xlabel('Minutes')\n",
    "plt.ylabel('Cosine Similarity')\n",
    "plt.suptitle('Cosine Similarity of Sequential Transcript Segments')\n",
    "plt.title('Week 4 - Lesson-4-1: Probabilistic Retrieval Model Basic Idea')\n",
    "\n",
    "ax.xaxis.set_major_locator(MinuteLocator(byminute=range(0, 60), interval=2))\n",
    "ax.xaxis.set_major_formatter(DateFormatter('%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calc Breakpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtopics_corpuses: Dict[int, Corpus] = dict()\n",
    "breakpoints_list: Dict[int, Corpus] = dict()\n",
    "for interval in time_intervals:\n",
    "    corpus = corpus_times[interval]\n",
    "    breakpoints = calc_breakpoints(corpus.ts_cos_similarity)\n",
    "    print(f'Breakpoints at {interval}-Second Interval:', breakpoints)\n",
    "    breakpoints_list[interval] = breakpoints\n",
    "    subtopics_documents = merge_documents_breakpoints(corpus.documents, breakpoints)\n",
    "    topic_transitions_corpus = Corpus(corpus.vocab, subtopics_documents)\n",
    "    topic_transitions_corpus.create_term_doc_freq_matrix()\n",
    "    topic_transitions_corpus.calc_similarity_ts()\n",
    "\n",
    "    # add to dict\n",
    "    subtopics_corpuses[interval] = topic_transitions_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_times[60].documents[4:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtopics_corpuses[60].documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interval = 60\n",
    "# fig, axes = plt.subplots(1,3, figsize = (12,4))\n",
    "# for i, interval in enumerate(time_intervals):\n",
    "#     axes[i].plot(make_x_values(interval), corpus_times[interval].ts_cos_similarity)\n",
    "#     axes[i].vlines((np.array(breakpoints_list[interval]))* interval, [0] * len(breakpoints_list[interval]), [1] * len(breakpoints_list[interval]), colors='k', linestyles='dashed')\n",
    "#     axes[i].set_ylim(0, 0.65)\n",
    "# plt.xlabel('Seconds')\n",
    "# plt.ylabel('Cosine Similarity')\n",
    "# plt.suptitle('Cosine Similarity of Sequential Transcript Segments')\n",
    "# axes[0].title('Week 4 - Lesson-4-1: Probabilistic Retrieval Model Basic Idea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize = (14, 4))\n",
    "for i, interval in enumerate(time_intervals[1:]):   # skip 5 second interval\n",
    "    axes[i].plot(make_x_values(corpus_times[interval].documents), corpus_times[interval].ts_cos_similarity)\n",
    "    axes[i].vlines([corpus_times[interval].documents[b].end for b in breakpoints_list[interval]], [0] * len(breakpoints_list[interval]), [1] * len(breakpoints_list[interval]), colors='k', linestyles='dashed')\n",
    "    axes[i].set_ylim(0, 0.65)\n",
    "    axes[i].set_xlim(datetime.datetime(1900, 1, 1, 0, 0, 0), datetime.datetime(1900, 1, 1, 0, 13, 0),)\n",
    "    # plt.legend(['30', '45', '60'], title='Time Intervals (Seconds)')\n",
    "    axes[i].set_xlabel('Minutes')\n",
    "    axes[i].set_ylabel('Cosine Similarity')\n",
    "    plt.suptitle('Cosine Similarity of Sequential Transcript Segments with Detected Breakpoints')\n",
    "    axes[i].set_title(f'Time Interval: {interval} Seconds')\n",
    "\n",
    "    axes[i].xaxis.set_major_locator(MinuteLocator(byminute=range(0, 60), interval=2))\n",
    "    axes[i].xaxis.set_major_formatter(DateFormatter('%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Breakpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtopic_corpus = subtopics_corpuses[30]\n",
    "\n",
    "\n",
    "\n",
    "# raw_subtopic_label_map = utils.make_segment_label_mapping(subtopic_corpus.vocab.transcript_segements, subtopic_corpus.documents)\n",
    "# print(len(raw_subtopic_label_map))\n",
    "# print(raw_subtopic_label_map)\n",
    "\n",
    "# naive_time_label_map = utils.make_segment_label_mapping(subtopic_corpus.vocab.transcript_segements, corpus_times[60].documents)\n",
    "# print(len(naive_time_label_map))\n",
    "# print(naive_time_label_map)\n",
    "\n",
    "# from sklearn.metrics import silhouette_score\n",
    "# doc_term_matrix_raw_segments = Corpus(vocab, vocab.transcript_segements)\n",
    "# doc_term_matrix_raw_segments.create_term_doc_freq_matrix()\n",
    "# print(silhouette_score(doc_term_matrix_raw_segments.term_doc_freq_matrix.T, raw_subtopic_label_map))\n",
    "# print(silhouette_score(doc_term_matrix_raw_segments.term_doc_freq_matrix.T, naive_time_label_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtopics_corpuses[30].documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide raw transcript into 10 second intervals\n",
    "base_corpus = Corpus(vocab, utils.merge_documents_time_interval(vocab.transcript_segements, 10))\n",
    "base_corpus.create_term_doc_freq_matrix()\n",
    "\n",
    "# create a mapping from 10 second interval to 60 second intervals\n",
    "naive_time_label_map = utils.make_segment_label_mapping(base_corpus.documents, corpus_times[60].documents)\n",
    "print(naive_time_label_map)\n",
    "print(silhouette_score(base_corpus.term_doc_freq_matrix.T, naive_time_label_map)) # naive, 60 second topic transition score\n",
    "print()\n",
    "\n",
    "for interval in time_intervals:\n",
    "\n",
    "    # create a mapping from 10 second intervals to model predicted topic transitions\n",
    "    subtopic_label_map = utils.make_segment_label_mapping(base_corpus.documents, subtopics_corpuses[interval].documents)\n",
    "    print(subtopic_label_map)\n",
    "\n",
    "    # calculate silhouettes scores\n",
    "    print(silhouette_score(base_corpus.term_doc_freq_matrix.T, subtopic_label_map))   # model topic transition score\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('tis-project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85a5c2dc979ec021335ff2758cc7fbdc3d2baa50f6cf88c303f22e6e76e98821"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
